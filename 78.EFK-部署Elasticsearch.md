### 部署Elasticsearch
#### 1、创建名称空间
```shell script
[root@efk-k8s-master01 efk]# pwd
/root/efk
[root@efk-k8s-master01 efk]# vim kube-logging.yaml
```
```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: kube-logging
```
```shell script
[root@efk-k8s-master01 efk]# kubectl apply -f kube-logging.yaml 
namespace/kube-logging created
```
#### 2、创建 headless service 服务
```shell script
[root@efk-k8s-master01 efk]# vim elasticsearch_svc.yaml 
```
```yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch
  namespace: kube-logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
    - port: 9200
      name: rest
    - port: 9300
      name: inter-node
```
```shell script
[root@efk-k8s-master01 efk]# kubectl apply -f elasticsearch_svc.yaml
service/elasticsearch created
```
#### 3、安装nfs
```shell script
[root@efk-k8s-master01 ~]# yum install nfs-utils -y
[root@efk-k8s-node01 ~]# yum install nfs-utils -y
```
```shell script
[root@efk-k8s-master01 ~]#  systemctl start nfs
[root@efk-k8s-node01 ~]#  systemctl start nfs
```
```shell script
[root@efk-k8s-master01 ~]# systemctl enable nfs.service
[root@efk-k8s-node01 ~]# systemctl enable nfs.servic
```
#### 4、在master01上创建nfs共享目录
```shell script
[root@efk-k8s-master01 ~]# mkdir /data/v1 -p
[root@efk-k8s-master01 ~]# vim /etc/exports
/data/v1 172.16.120.0/24(rw,no_root_squash)
```
```shell script
[root@efk-k8s-master01 ~]#  exportfs -arv
exporting 172.16.120.0/24:/data/v1
[root@efk-k8s-master01 ~]# systemctl restart nfs
```
#### 5、创建 nfs 作为存储的供应商
##### 5.1 在master1，创建 sa
```shell script
[root@efk-k8s-master01 ~]# cd efk/
[root@efk-k8s-master01 efk]# vim serviceaccount.yaml
```
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-provisioner
```
```shell script
[root@efk-k8s-master01 efk]# kubectl apply -f serviceaccount.yaml
```
##### 5.2 在master1，对sa做rbac授权
```shell script
[root@efk-k8s-master01 efk]# vim rbac.yaml
```
```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
  - apiGroups: [""]
    resources: ["services", "endpoints"]
    verbs: ["get"]
  - apiGroups: ["extensions"]
    resources: ["podsecuritypolicies"]
    resourceNames: ["nfs-provisioner"]
    verbs: ["use"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-provisioner
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-provisioner
  apiGroup: rbac.authorization.k8s.io
```
```shell script
[root@efk-k8s-master01 efk]# kubectl apply -f rbac.yaml
clusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-provisioner created
```
#### 5.3 k8s 1.20以后的版本，需要加参数，并重启api-server
```shell script
[root@efk-k8s-master01 efk]# vim /etc/kubernetes/manifests/kube-apiserver.yaml
```
```shell script
- --feature-gates=RemoveSelfLink=false
```
![image](https://github.com/498946975/DevOps/blob/master/images/efk03.png)
```shell script
删除没用的kube-apiserver
[root@efk-k8s-master01 efk]# kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml
pod/kube-apiserver created
[root@efk-k8s-master01 efk]# kubectl get pods -n kube-system | grep apiserver
kube-apiserver                             0/1     Pending   0          8s
kube-apiserver-efk-k8s-master01            0/1     Running   0          8s
[root@efk-k8s-master01 efk]# kubectl delete pods kube-apiserver -n kube-system
pod "kube-apiserver" deleted
```
#### 6、部署nfs供应商
```shell script
[root@efk-k8s-master01 efk]# vim nfs-deployment.yaml
```
```yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-provisioner
spec:
  selector:
    matchLabels:
      app: nfs-provisioner
  replicas: 1
  strategy:   # 更新策略
    type: Recreate    # 重建
  template:
    metadata:
      labels:
        app: nfs-provisioner
    spec:
      serviceAccount: nfs-provisioner # 指定sa的名称，并且这个sa进行了rbac授权
      containers:
        - name: nfs-provisioner
          image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: nfs-client-root   # 下面指定的volumes
              mountPath: /persistentvolumes # 挂在到容器内的目录
          env:
            - name: PROVISIONER_NAME
              value: example.com/nfs  # 指定供应商的环境变量，和下面创建的存储类必须一致
            - name: NFS_SERVER
              value: 172.16.120.77
            - name: NFS_PATH
              value: /data/v1
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.16.120.77
            path: /data/v1
```
#### 7、创建存储类，然后根据上面的deplyment，自动划分pv和pvc
```shell script
[root@efk-k8s-master01 efk]# vim class.yaml
```
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: do-block-storage
provisioner: example.com/nfs  # 指定供应商是example.com/nfs，需要和上面的deployment里面的env：PROVISIONER_NAME一致
```
```shell script
[root@efk-k8s-master01 efk]# kubectl apply -f class.yaml 
storageclass.storage.k8s.io/do-block-storage created
```
```shell script
[root@efk-k8s-master01 efk]# kubectl get storageclass 
NAME               PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION         AGE
do-block-storage   example.com/nfs   Delete(回收策略)     Immediate(卷绑定模式)    false(是否扩展)            109s
```
#### 8、用statefulset安装Elasticsearch
```shell script
[root@efk-k8s-master01 efk]# vim elasticsearch-statefulset.yaml
```
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: kube-logging
spec:
  serviceName: elasticsearch    # 指定svc的name，已经创建好的
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 1000m  # 1核cpu
            requests:
              cpu: 100m   # 空闲至少有0。1核心
        ports:
        - containerPort: 9200   # 容器暴露端口
          name: rest    #9200端口别名
          protocol: TCP
        - containerPort: 9300    # 容器暴露端口
          name: inter-node  #9300端口别名
          protocol: TCP
        volumeMounts:
        - name: data  # 下面指定的volumeClaimTemplates
          mountPath: /usr/share/elasticsearch/data  # 挂载到容器中的目录
        env:
          - name: cluster.name  # es集群的名称
            value: k8s-logs
          - name: node.name   # 节点的名称，通过 metadata.name 来获取。这将解析为 es-cluster-[0,1,2]
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts  # 下面是全质量域名，跟上面的创建的replicas的个数必须一致
            value: "es-cluster-0.elasticsearch.kube-logging.svc.cluster.local,es-cluster-1.elasticsearch.kube-logging.svc.cluster.local,es-cluster-2.elasticsearch.kube-logging.svc.cluster.local"
          - name: cluster.initial_master_nodes  # 下面pod的名称，跟上面的创建的replicas的个数必须一致
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m"
      initContainers:   # 初始化容器
      - name: fix-permissions
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true  # 开启特权模式
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
  volumeClaimTemplates:   # 卷申请模板
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]    # 读写规则
      storageClassName: do-block-storage    # 去do-block-storage申请pv，do-block-storage是上面才创建的storageclass
      resources:
        requests:
          storage: 10Gi   # 申请10G
```
```shell script
[root@efk-k8s-master01 efk]# kubectl apply -f elasticsearch-statefulset.yaml 
statefulset.apps/es-cluster created
```
```shell script
[root@efk-k8s-master01 efk]# kubectl get pod -n kube-logging
NAME           READY   STATUS    RESTARTS   AGE
es-cluster-0   1/1     Running   0          9m13s
es-cluster-1   1/1     Running   0          6m7s
es-cluster-2   1/1     Running   0          6m

[root@efk-k8s-master01 efk]# kubectl get svc -n kube-logging   
NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE
elasticsearch   ClusterIP   None         <none>        9200/TCP,9300/TCP   60m
```
```shell script
自动生成pv和pvc
[root@efk-k8s-master01 efk]# kubectl get pv 
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS       REASON   AGE
pvc-82bd98dc-c591-4803-a327-3b82876800cc   10Gi       RWO            Delete           Bound    kube-logging/data-es-cluster-1   do-block-storage            17m
pvc-9cdd8aa3-ad0a-47fe-a992-4a36eb7be51b   10Gi       RWO            Delete           Bound    kube-logging/data-es-cluster-2   do-block-storage            17m
pvc-c73dc7db-f44f-4ec6-a46c-079b7c7ba185   10Gi       RWO            Delete           Bound    kube-logging/data-es-cluster-0   do-block-storage            20m

[root@efk-k8s-master01 efk]# kubectl get pvc -A
NAMESPACE      NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
kube-logging   data-es-cluster-0   Bound    pvc-c73dc7db-f44f-4ec6-a46c-079b7c7ba185   10Gi       RWO            do-block-storage   20m
kube-logging   data-es-cluster-1   Bound    pvc-82bd98dc-c591-4803-a327-3b82876800cc   10Gi       RWO            do-block-storage   17m
kube-logging   data-es-cluster-2   Bound    pvc-9cdd8aa3-ad0a-47fe-a992-4a36eb7be51b   10Gi       RWO            do-block-storage   17m
```
### 9、临时把9200端口映射到物理机上，测试一下
```shell script
[root@efk-k8s-master01 efk]# kubectl port-forward es-cluster-0 9200:9200 --namespace=kube-logging
Forwarding from 127.0.0.1:9200 -> 9200
Forwarding from [::1]:9200 -> 920
```
```shell script
[root@efk-k8s-master01 ~]# curl http://localhost:9200/_cluster/state?pretty >> 1.txt
```