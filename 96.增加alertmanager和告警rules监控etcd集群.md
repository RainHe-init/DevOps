### 增加alertmanager和rules监控etcd集群
#### 1、增加alertmanager的configmap
```shell script
[root@SpringCloud-k8s-master01 prometheus]# pwd
/root/prometheus
[root@SpringCloud-k8s-master01 prometheus]# vim alertmanager-cm.yaml
```
```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: alertmanager
  namespace: monitor-sa
data:
  alertmanager.yml: |-
    global:
      resolve_timeout: 1m 
      smtp_smarthost: 'smtp.163.com:25'   # 邮箱设置中的，smtp服务器地址，163 邮箱的 SMTP 服务器地址+端口
      smtp_from: 'q498946975@163.com'   # 发送告警用到的邮箱
      smtp_auth_username: 'q498946975'  # 发送有限的认证用户
      smtp_auth_password: 'XXXXXXXX'
      smtp_require_tls: false
    route:    #用于设置告警的分发策略
      group_by: [alertname]   # alertmanager 会根据 group_by 配置将 Alert 分组
      group_wait: 10s     # 分组等待时间。也就是告警产生后等待 10s，如果有同组告警一起发出
      group_interval: 10s # 上下两组发送告警的间隔时间
      repeat_interval: 10m    # 重复发送告警的时间，减少相同邮件的发送频率，默认是 1h
      receiver: default-receiver  #定义谁来收告警
    receivers:
    - name: 'default-receiver'
      email_configs:
      - to: '498946975@qq.com'  # to 后面指定发送到哪个邮箱
        send_resolved: true
```
```shell script
[root@SpringCloud-k8s-master01 prometheus]# kubectl apply -f alertmanager-cm.yaml
```
#### 2、增加增加alertmanager的svc
```shell script
[root@SpringCloud-k8s-master01 prometheus]# pwd
/root/prometheus
[root@SpringCloud-k8s-master01 prometheus]# vim alertmanager-svc.yaml 
```
```yaml
---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: prometheus
    kubernetes.io/cluster-service: 'true'
  name: alertmanager
  namespace: monitor-sa
spec:
  ports:
  - name: alertmanager
    nodePort: 30066
    port: 9093
    protocol: TCP
    targetPort: 9093
  selector:
    app: prometheus
  sessionAffinity: None
  type: NodePort
```
#### 3、修改prometheus的configmap，增加rules配置
```shell script
[root@SpringCloud-k8s-master01 prometheus]# pwd
/root/prometheus
root@SpringCloud-k8s-master01 prometheus]# vim prometheus-alertmanager-configmap.yaml
```
```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    app: prometheus
  name: prometheus-config
  namespace: monitor-sa
data:
  prometheus.yml: |
    rule_files:
    - /etc/prometheus/rules.yml   #写入告警规则
    alerting:
      alertmanagers:
      - static_configs:
        - targets: ["localhost:9093"]
    global:
      scrape_interval: 15s
      scrape_timeout: 10s
      evaluation_interval: 1m
    scrape_configs:
    - job_name: 'ca-etcd'   # 新增etcd监控
      metrics_path: /metrics
      scheme: https
      tls_config:
        insecure_skip_verify: true
        ca_file: /prometheus/pki/ca.pem
        key_file: /prometheus/pki/etcd-key.pem
        cert_file: /prometheus/pki/etcd.pem
      static_configs:
      - targets: ['172.16.120.200:22379','172.16.120.201:22379','172.16.120.202:22379']
  rules.yml: |
    groups:
    - name: example
      rules:
      - alert: etcd的cpu使用率大于80%
        expr: rate(process_cpu_seconds_total{job=~"ca-etcd"}[1m]) * 100 > 80
        for: 2s
        labels:
          severity: warnning
        annotations:
          description: "{{$labels.instance}}的{{$labels.job}}组件的cpu使用率超过80%"
      - alert:  etcd的cpu使用率大于90%
        expr: rate(process_cpu_seconds_total{job=~"ca-etcd"}[1m]) * 100 > 90
        for: 2s
        labels:
          severity: critical
        annotations:
          description: "{{$labels.instance}}的{{$labels.job}}组件的cpu使用率超过90%"
      - alert: kubernetes-etcd打开句柄数>600
        expr: process_open_fds{job=~"ca-etcd"}  > 600
        for: 2s
        labels:
          severity: warnning
        annotations:
          description: "{{$labels.instance}}的{{$labels.job}}打开句柄数>600"
          value: "{{ $value }}"
      - alert: kubernetes-etcd打开句柄数>1000
        expr: process_open_fds{job=~"ca-etcd"}  > 1000
        for: 2s
        labels:
          severity: critical
        annotations:
          description: "{{$labels.instance}}的{{$labels.job}}打开句柄数>1000"
          value: "{{ $value }}"
      - alert: vitrual-memory-etcd
        expr: process_virtual_memory_bytes{job=~"ca-etcd"}  > 2000000000
        for: 2s
        labels:
          severity: warnning
        annotations:
          description: "组件{{$labels.job}}({{$labels.instance}}): 使用虚拟内存超过2G"
          value: "{{ $value }}"
      - alert: Etcd_leader
        expr: etcd_server_has_leader{job="ca-etcd"} == 0
        for: 2s
        labels:
          team: admin
        annotations:
          description: "组件{{$labels.job}}({{$labels.instance}}): 当前没有leader"
          value: "{{ $value }}"
          threshold: "0"
      - alert: Etcd_leader_changes
        expr: rate(etcd_server_leader_changes_seen_total{job="ca-etcd"}[1m]) > 0
        for: 2s
        labels:
          team: admin
        annotations:
          description: "组件{{$labels.job}}({{$labels.instance}}): 当前leader已发生改变"
          value: "{{ $value }}"
          threshold: "0"
      - alert: Etcd_failed
        expr: rate(etcd_server_proposals_failed_total{job="ca-etcd"}[1m]) > 0
        for: 2s
        labels:
          team: admin
        annotations:
          description: "组件{{$labels.job}}({{$labels.instance}}): 服务失败"
          value: "{{ $value }}"
          threshold: "0"
      - alert: Etcd_db_total_size
        expr: etcd_debugging_mvcc_db_total_size_in_bytes{job="ca-etcd"} > 10000000000
        for: 2s
        labels:
          team: admin
        annotations:
          description: "组件{{$labels.job}}({{$labels.instance}})：db空间超过10G"
          value: "{{ $value }}"
          threshold: "10G"
```
```shell script
[root@SpringCloud-k8s-master01 prometheus]# kubectl apply -f prometheus-alertmanager-configmap.yaml
```
#### 4、修改prometheus的部署文件
```shell script
[root@SpringCloud-k8s-master01 prometheus]# pwd
/root/prometheus
[root@SpringCloud-k8s-master01 prometheus]# vim prometheus-alertmanager-deploy.yaml 
```
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-server
  namespace: monitor-sa
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
      component: server
    #matchExpressions:
    #- {key: app, operator: In, values: [prometheus]}
    #- {key: component, operator: In, values: [server]}
  template:
    metadata:
      labels:
        app: prometheus
        component: server
      annotations:
        prometheus.io/scrape: 'false'
    spec:
      #nodeName: k8s-node01
      serviceAccountName: monitor
      containers:
      - name: prometheus
        image: prom/prometheus:v2.2.1
        imagePullPolicy: IfNotPresent
        command:
        - "/bin/prometheus"
        args:
        - "--config.file=/etc/prometheus/prometheus.yml"
        - "--storage.tsdb.path=/prometheus"
        - "--storage.tsdb.retention=24h"
        - "--web.enable-lifecycle"
        ports:
        - containerPort: 9090
          protocol: TCP
        volumeMounts:
        - mountPath: /etc/prometheus    # 存prometheus的配置的
          name: prometheus-config
        - mountPath: /prometheus/     # 存prometheus的数据的
          name: prometheus-storage-volume
        - name: pki
          mountPath: /prometheus/pki/   # 存ca-etcd证书的
################
      - name: alertmanager
        image: prom/alertmanager:v0.14.0
        imagePullPolicy: IfNotPresent
        args:
        - "--config.file=/etc/alertmanager/alertmanager.yml"
        - "--log.level=debug"
        ports:
        - containerPort: 9093
          protocol: TCP
          name: alertmanager
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        - name: alertmanager-storage
          mountPath: /alertmanager
        - name: localtime
          mountPath: /etc/localtime
      volumes:
        - name: prometheus-config
          configMap:
            name: prometheus-config
            items:
              - key: prometheus.yml
                path: prometheus.yml
                mode: 0644
              - key: rules.yml
                path: rules.yml
                mode: 0644
        - name: prometheus-storage-volume
          hostPath:
           path: /data
           type: Directory
        - name: alertmanager-config
          configMap:
            name: alertmanager
        - name: alertmanager-storage
          hostPath:
           path: /data/alertmanager
           type: DirectoryOrCreate    #有就用有的，没有就自动创建一个
        - name: localtime
          hostPath:
           path: /usr/share/zoneinfo/Asia/Shanghai
        - name: pki   # 95号文章，第三部挂载的证书
          configMap:
            name: pki
            items:
            - key: ca.pem
              path: ca.pem
            - key: etcd.pem
              path: etcd.pem
            - key: etcd-key.pem
              path: etcd-key.pem
            defaultMode: 420
```
```shell script
[root@SpringCloud-k8s-master01 prometheus]# kubectl apply -f prometheus-alertmanager-deploy.yaml
```
#### 5、查看一下pod状态
```shell script
[root@SpringCloud-k8s-master01 prometheus]# kubectl get pod -n monitor-sa
NAME                                 READY   STATUS    RESTARTS   AGE
node-exporter-2k49b                  1/1     Running   0          3d5h
node-exporter-4422g                  1/1     Running   0          3d5h
prometheus-server-5cf756544b-4wmf5   2/2     Running   0          172m
```
#### 6、创建grafana的defployment和svc
```shell script
[root@SpringCloud-k8s-master01 prometheus]# pwd
/root/prometheus
[root@SpringCloud-k8s-master01 prometheus]# vim grafana-deployment.yaml 
```
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-grafana
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      task: monitoring
      k8s-app: grafana
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: grafana
    spec:
      containers:
      - name: grafana
        image: k8s.gcr.io/heapster-grafana-amd64:v5.0.4
        ports:
        - containerPort: 3000
          protocol: TCP
        volumeMounts:
        - mountPath: /etc/ssl/certs
          name: ca-certificates
          readOnly: true
        - mountPath: /var
          name: grafana-storage
        env:
        - name: INFLUXDB_HOST
          value: monitoring-influxdb
        - name: GF_SERVER_HTTP_PORT
          value: "3000"
          # The following env variables are required to make Grafana accessible via
          # the kubernetes api-server proxy. On production clusters, we recommend
          # removing these env variables, setup auth for grafana, and expose the grafana
          # service using a LoadBalancer or a public IP.
        - name: GF_AUTH_BASIC_ENABLED
          value: "false"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ORG_ROLE
          value: Admin
        - name: GF_SERVER_ROOT_URL
          # If you're only using the API Server proxy, set this value instead:
          # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy
          value: /
      volumes:
      - name: ca-certificates
        hostPath:
          path: /etc/ssl/certs
      - name: grafana-storage
        emptyDir: {}
```
```shell script
[root@SpringCloud-k8s-master01 prometheus]# pwd
/root/prometheus
[root@SpringCloud-k8s-master01 prometheus]# cat grafana-svc.yaml 
```
```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)
    # If you are NOT using this as an addon, you should comment out this line.
    kubernetes.io/cluster-service: 'true'
    kubernetes.io/name: monitoring-grafana
  name: monitoring-grafana
  namespace: kube-system
spec:
  # In a production setup, we recommend accessing Grafana through an external Loadbalancer
  # or through a public IP.
  # type: LoadBalancer
  # You could also use NodePort to expose the service at a randomly-generated port
  # type: NodePort
  ports:
  - port: 80
    targetPort: 3000
  selector:
    k8s-app: grafana
  type: NodePort
```
#### 7、具体调试grafana的类似操作，见48号文档
#### 8、调试好的grafana的json文件
```shell script
在install package文件夹中 ca_etcd_dashboard_by_liuxiang.json
```