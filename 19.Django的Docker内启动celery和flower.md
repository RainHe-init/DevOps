### 在Docker中启动celery，supervisord，flower，保障Django的多线程和定时任务的执行
详细信息请参考BookManager的github
https://github.com/498946975/BookManager
#### 1、在pycharm中安装flower、supervisord、celery
```shell script
flower==0.9.5
celery==4.4.1
supervisor==4.2.0
redis==3.4.1
以上这几个版本配合比较稳定,已经在生产环境中测试过
```
#### 2、在Django中更新requirements.txt
```shell script
pip freeze > requirements.txt
```
#### 3、自动生成supervisor配置，在Django的主目录生成
```shell script
echo_supervisord_conf > /Users/oliver/Documents/PyCharm/BookManager/supervisord.conf

(venv) Oswald:BookManager oliver$ ls -l
total 48
drwxr-xr-x   8 oliver  staff   256 Jun 25 18:39 BookManager
-rw-r--r--   1 oliver  staff   476 Jul  1 09:57 Dockerfile
-rw-r--r--   1 oliver  staff  1431 Jul  3 15:06 Jenkinsfile
drwxr-xr-x   3 oliver  staff    96 Jan  1  2021 __pycache__
drwxr-xr-x  12 oliver  staff   384 Jul  7 20:04 app001
-rw-r--r--   1 oliver  staff   300 May 25 16:09 arango.py
-rwxr-xr-x   1 oliver  staff   667 Nov 29  2020 manage.py
drwxr-xr-x   8 oliver  staff   256 Jul  7 20:43 mycelery
-rw-r--r--   1 oliver  staff   644 Jul  7 20:54 requirements.txt
drwxr-xr-x   4 oliver  staff   128 Jan  9 14:48 static
-rw-r--r--   1 oliver  staff   923 Jul  7 20:52 supervisord.conf
drwxr-xr-x  12 oliver  staff   384 May 25 16:09 templates
drwxr-xr-x   7 oliver  staff   224 Dec 20  2020 venv
```
#### 4、修改supervisord.conf配置文件部分信息
```editorconfig
[unix_http_server]
file=/tmp/supervisor.sock

[program:celery.beat]
command=celery -A celery_tasks.main beat -l info
numprocs=1
autostart=true
autorestart=true
redirect_stderr=true
stdout_logfile=/code/celery_beat_out.log

[program:celery.worker]
command=celery -A celery_tasks.main worker -l info
autostart=true
autorestart=true
startretries=3
stopwaitsecs=600
redirect_stderr=true
stdout_logfile=/code/celery_worker_out.log

[program:celery.flower]
command=flower -A celery_tasks.main --port=5000
numprocs=1
autostart=true
autorestart=true
startretries=3
redirect_stderr=true

[supervisord]
logfile=/tmp/supervisord.log
logfile_maxbytes=50MB
logfile_backups=10
loglevel=info
pidfile=/tmp/supervisord.pid
nodaemon=false
silent=false
minfds=1024
minporcs=200


[rpcinterface:supervisor]
supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface


[supervisorctl]
serverurl=unix:///tmp/supervisor.sock
```
#### 5、celery的配置文件如下
```python
"""
celery配置文件
"""
from celery.schedules import crontab
from datetime import timedelta
from kombu import Queue, Exchange

# broker_url = 'redis://127.0.0.1:6379/15'  # 消息中间件数据库
# result_backend = 'redis://127.0.0.1:6379/14'  # 异步结果数据库
ENV = "127.0.0.1:6379"
BROKER_URL = f"redis://{ENV}/12"  # 消息中间件数据库
CELERY_RESULT_BACKEND = f"redis://{ENV}/13"  # 指结果的接收地址
CELERY_FORCE_EXECV = True  # 非常重要,有些情况下可以死锁
CELERY_CREATE_MISSING_QUEUES = True  # 某个程序中出现的队列,在broker中不存在,则立刻创建它
CELERY_ACKS_LATE = True  # 任务失败允许重试,对性能会稍有影响

"""结果相关"""
CELERY_TASK_SERIALIZER = "json"  # 指定任务序列化方式
CELERY_RESULT_SERIALIZER = "json"  # 指定结果序列化方式
CELERY_MESSAGE_COMPRESSION = "zlib"  # 压缩方案选择,可以是zlib,bzip2,默认是发送没有压缩的数据
CELERY_ACCEPT_CONTENT = ['json']  # Ignore other content
CELERY_TIME_ZONE = "Asia/Shanghai"  # 设置时区
CELERY_ENABLE_UTC = True  # 启动时区设置 celery使用的是utc时区

"""并发相关"""
CELERY_REDIS_MAX_CONNECTIONS = 4  # celery worker每次去redis取任务的数,默认值就是4
CELERYD_CONCURRENCY = 20  # 并发worker数,一般根据cpu数里来设置
BROKER_TRANSPORT_OPTIONS = {'vi sibility_timeo': 5}  # 5min
CELERY_ANNOTATIONS = {"*": {'rate_limit': '10/s'}}  # 限制所有的任务的刷新频率
CELERY_TASK_RESULT_EXPIRES = 15 * 60  # 任务结果过期设置
CELERYD_TASK_TIME_LIMIT = 15 * 60  # 单个任务运行的最大时间,超过这个时间,task就会被kill,自动交给父进程
CELERYD_MAX_TASKS_PER_CHILD = 40  # 每个workerwor最多执行的任务数,超过这个就将进行销毁，防止内存泄漏
CELERY_DISABLE_RATE_LIMITS = True  # 任务发出后,经过一段时间还未收到acknowLedge,就将任务重新交给其他worker执行,关闭限速

# 任务队列,防止相互影响
CELERY_QUEUES = (  # 设置execute队列,绑定routing_key
    Queue('default', routing_key='default'),
    Queue('RequestMagnify_historyAPI_queve', Exchange('for_RequestMagnify_historyAPI_crontab'),
          routing_key='for_RequestMagnify_historyAPI_router'),
    # Queue('更mysql新_queve', Exchange('for_更新mysqlcrontab'), routing_key = 'for_更新mysqL_router'),
)

# 异步任务
CELERY_ROUTES = {
    "RequestMagnify_historyAPI": {
        "queue": "RequestMagnify_historyAPI_queue",
        "routing_key": "for_RequestMagnify_historyAPI_router"
    }
}

# 定时任务
CELERYBEAT_SCHEDULE = {
    "overview_every_1d": {
        "task": "更新mysql",
        "schedule": crontab(minute=0, hour=3, day_of_week="*/1"),  # 每天执行一次
        "args": (),  # 参数
        "options": {'queve': '更新mysqL_queue', 'routing_key': 'for_更新mysqL_router'}
    }
}

```
#### 6、对于部署来说，最重要的还是安装上面的几个包，然后重新打docker镜像即可
#### 7、对于flower程序，可以通过web访问，上面的flower配置在容器中是5000端口，所以，在宿主机上还需要暴露5000端口

